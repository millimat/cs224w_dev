{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snap\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = 1000\n",
    "def filter_subreddits(subreddit_metagraph):\n",
    "    mg = snap.TNEANet.Load(snap.TFIn(subreddit_metagraph))\n",
    "    srids_to_names = {}\n",
    "\n",
    "    # Get the largest |top| subreddits by subscriber count\n",
    "    acceptable = sorted(((NI.GetId(), mg.GetIntAttrDatN(NI, 'subscribers')) for NI in mg.Nodes()),\n",
    "                        key=(lambda (nid, subs): subs), reverse=True)[:top]\n",
    "    acceptable = set(nid for (nid, subs) in acceptable)\n",
    "\n",
    "    for NI in mg.Nodes():\n",
    "        # t5_xxxxx -> politics\n",
    "        if NI.GetId() in acceptable:\n",
    "            srids_to_names[mg.GetStrAttrDatN(NI, 'name')] = mg.GetStrAttrDatN(NI, 'url')[3:-1]\n",
    "\n",
    "    return srids_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit_metagraph = 'subreddits.graph'\n",
    "srids_to_srnames = filter_subreddits(subreddit_metagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_submissions(srids_to_srnames, submission_directory, outsubname):\n",
    "    outfile = open(outsubname, 'w')\n",
    "    outfile.write('\\t'.join(['Author', 'Subreddit', 'Score', 'Gilded', 'Created_UTC', 'Is_Self',\n",
    "                           'Title', 'Selftext']) + '\\n')\n",
    "\n",
    "    postids_to_authors = {}\n",
    "\n",
    "    subfiles = []\n",
    "    for fname in os.listdir(submission_directory):\n",
    "        if fname.startswith('.') or fname.endswith('~'):\n",
    "            continue\n",
    "        path = os.path.join(submission_directory, fname)\n",
    "        if os.path.isdir(path): # skip subdirectories\n",
    "            continue\n",
    "        subfiles.append(path)\n",
    "        \n",
    "    print('Parsing submissions...')\n",
    "    for subfile in subfiles:\n",
    "        print(subfile + ': parsing...')\n",
    "        submissions = (json.loads(line) for line in bz2.BZ2File(subfile))\n",
    "        for (i, sub) in enumerate(submissions):\n",
    "            if sub['subreddit_id'] in srids_to_srnames: # subreddit_id is t5_xxxxx\n",
    "                post_id = 't3_' + sub['id']\n",
    "                author = sub['author'].lower()\n",
    "                subreddit = srids_to_srnames[sub['subreddit_id']]\n",
    "                score = sub['score']\n",
    "                gold = sub.get('gilded', 0)\n",
    "                timestamp = sub['created_utc']\n",
    "                is_self = int(sub['is_self'])\n",
    "                title = re.sub(r'\\s+', ' ', sub['title'].encode('ascii', 'backslashreplace')).lower()\n",
    "                selftext = '' if not is_self \\\n",
    "                           else re.sub(r'\\s+', ' ', sub['selftext'].encode('ascii', 'backslashreplace')).lower()\n",
    "\n",
    "                postids_to_authors[post_id] = author\n",
    "                outfile.write('\\t'.join(str(x) for x in [post_id, author, subreddit, score, gold, timestamp,\n",
    "                                                         is_self, title, selftext]) + '\\n')\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(i)\n",
    "                \n",
    "    outfile.close()\n",
    "\n",
    "    return postids_to_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing submissions...\n",
      "submissions_jan2012/RS_2012-01.bz2: parsing...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n"
     ]
    }
   ],
   "source": [
    "submission_directory = 'submissions_jan2012'\n",
    "output_submission_text_file = 'reddit_submissions_jan2012.txt'\n",
    "postids_to_authors = parse_submissions(srids_to_srnames, submission_directory, output_submission_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_comments(srids_to_srnames, postids_to_authors, comment_directory, outcomname):\n",
    "    outfile = open(outcomname, 'w')\n",
    "    outfile.write('#Commenter\\tCommentee\\tSubreddit\\tPost_ID\\tControversiality\\tUpvotes\\t' + \n",
    "                  'Downvotes\\tScore\\tGilded\\tCreated_UTC\\tIs_Reply\\tAuthor_Deleted\\t' + \n",
    "                  'Text_Deleted\\tComment_Text\\n')\n",
    "    \n",
    "    comids_to_authors = {} # t1_xxxxx -> gallowboob, e.g.\n",
    "        \n",
    "    comfiles = []\n",
    "    for fname in os.listdir(comment_directory):\n",
    "        if fname.startswith('.') or fname.endswith('~'):\n",
    "            continue\n",
    "        path = os.path.join(comment_directory, fname)\n",
    "        if os.path.isdir(path): # skip subdirectories\n",
    "            continue\n",
    "        comfiles.append(path)\n",
    "        \n",
    "    print('Parsing comments...')\n",
    "    for comfile in comfiles:        \n",
    "        print(comfile + ': parsing...')\n",
    "        comments = (json.loads(line) for line in bz2.BZ2File(comfile))\n",
    "        for (i, com) in enumerate(comments):\n",
    "            sr_id = com['subreddit_id'] # t5_xxxxx\n",
    "            post_id = com['link_id'] # t3_xxxxx\n",
    "            parent_id = com['parent_id'] # t1_xxxxx (comment) if reply; t3_xxxxx (post) if top-level comment\n",
    "            is_reply = parent_id.startswith('t1')\n",
    "            \n",
    "            if sr_id in srids_to_srnames and post_id in postids_to_authors\\\n",
    "            and (not is_reply or parent_id in comids_to_authors):\n",
    "                sr_name = srids_to_srnames[sr_id]\n",
    "                commenter = com['author'].lower()\n",
    "                parent_dict = comids_to_authors if is_reply else postids_to_authors\n",
    "                commentee = parent_dict[parent_id]\n",
    "                controversiality = com['controversiality']\n",
    "                upvotes = com['ups']\n",
    "                downvotes = com['downs']\n",
    "                score = com['score']\n",
    "                gilded = com['gilded']\n",
    "                created = com['created_utc']\n",
    "                body = re.sub(r\"\\s+\", ' ', com['body'].encode('ascii', 'backslashreplace')).lower()\n",
    "                author_deleted = int(commenter == '[deleted]')\n",
    "                text_deleted = int(body in ('[deleted]', '[removed]'))\n",
    "                \n",
    "                outfile.write('\\t'.join(str(x) for x in \n",
    "                              [commenter, commentee, sr_name, post_id, controversiality, upvotes,\n",
    "                               downvotes, score, gilded, created, int(is_reply), author_deleted,\n",
    "                               text_deleted, body]) + '\\n')\n",
    "                \n",
    "                comids_to_authors[com['name']] = commenter\n",
    "            \n",
    "            if i % 100000 == 0:\n",
    "                print(i) # Rudimentary progress indicator\n",
    "                \n",
    "    return comids_to_authors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing comments...\n",
      "comments_jan2012/RC_2012-01.bz2: parsing...\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8100000\n",
      "8200000\n",
      "8300000\n",
      "8400000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "8800000\n",
      "8900000\n",
      "9000000\n",
      "9100000\n",
      "9200000\n",
      "9300000\n",
      "9400000\n",
      "9500000\n",
      "9600000\n",
      "9700000\n",
      "9800000\n",
      "9900000\n",
      "10000000\n",
      "10100000\n",
      "10200000\n",
      "10300000\n",
      "10400000\n",
      "10500000\n",
      "10600000\n",
      "10700000\n",
      "10800000\n",
      "10900000\n",
      "11000000\n",
      "11100000\n",
      "11200000\n",
      "11300000\n",
      "11400000\n",
      "11500000\n",
      "11600000\n",
      "11700000\n",
      "11800000\n",
      "11900000\n",
      "12000000\n",
      "12100000\n",
      "12200000\n",
      "12300000\n",
      "12400000\n",
      "12500000\n",
      "12600000\n",
      "12700000\n",
      "12800000\n",
      "12900000\n",
      "13000000\n",
      "13100000\n",
      "13200000\n",
      "13300000\n",
      "13400000\n",
      "13500000\n",
      "13600000\n",
      "13700000\n",
      "13800000\n",
      "13900000\n",
      "14000000\n",
      "14100000\n",
      "14200000\n",
      "14300000\n",
      "14400000\n",
      "14500000\n",
      "14600000\n",
      "14700000\n",
      "14800000\n",
      "14900000\n",
      "15000000\n",
      "15100000\n",
      "15200000\n",
      "15300000\n",
      "15400000\n",
      "15500000\n",
      "15600000\n",
      "15700000\n",
      "15800000\n",
      "15900000\n",
      "16000000\n",
      "16100000\n",
      "16200000\n",
      "16300000\n"
     ]
    }
   ],
   "source": [
    "comment_directory = 'comments_jan2012'\n",
    "output_comment_text_file = 'reddit_comments_jan2012.txt'\n",
    "comids_to_authors = parse_comments(srids_to_srnames, postids_to_authors, comment_directory, output_comment_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12994655"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comids_to_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
