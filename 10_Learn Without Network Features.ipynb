{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import sklearn.linear_model, sklearn.ensemble, sklearn.model_selection \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srname_to_class = {}\n",
    "for (i,line) in enumerate(open('output/srurls_to_names.txt')):\n",
    "    url = line.strip().split()[0]\n",
    "    srname = url[3:-1] # e.g. '/r/politics/' to 'politics'\n",
    "    srname_to_class[srname] = np.float64(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_basic_language = np.genfromtxt('output/basic_and_language_nodelete.tsv', delimiter='\\t', skip_header=1,\n",
    "                                   converters = {2: lambda name: srname_to_class[name]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids = [line.strip().split('\\t')[1] for line in open('output/basic_and_language_nodelete.tsv').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = data_basic_language.shape[0]\n",
    "idx = np.array(range(m), dtype=int)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "data_basic_language = data_basic_language[idx,:]\n",
    "\n",
    "# Create training and test set\n",
    "trainprop = 0.95\n",
    "trainstop = int(m * trainprop)\n",
    "\n",
    "# ignore gold column at end and post id/user id columns at beginning\n",
    "trainset = data_basic_language[:trainstop]\n",
    "trainX = trainset[:,2:-2] \n",
    "trainY = trainset[:,-2]\n",
    "\n",
    "testset = data_basic_language[trainstop:]\n",
    "testX = testset[:,2:-2]\n",
    "testY = testset[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsizes = np.array(np.linspace(0, trainstop, 21)[1:], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first baseline model, we'll just make predictions using the overall mean sore from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45795\n",
      "91590\n",
      "137386\n",
      "183181\n",
      "228977\n",
      "274772\n",
      "320567\n",
      "366363\n",
      "412158\n",
      "457954\n",
      "503749\n",
      "549544\n",
      "595340\n",
      "641135\n",
      "686931\n",
      "732726\n",
      "778521\n",
      "824317\n",
      "870112\n",
      "915908\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    trainerrs.append(np.sqrt(np.mean((full_mean - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((full_mean - testY)**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Mean-only')\n",
    "plt.legend()\n",
    "plt.savefig('plots/mean_only.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll track the average deviation of post score from the overall mean for each user, each hour, each day of the week, and each subreddit. Each prediction will be the overall mean plus the sum of mean deviations for each relevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45795\n",
      "91590\n",
      "137386\n",
      "183181\n",
      "228977\n",
      "274772\n",
      "320567\n",
      "366363\n",
      "412158\n",
      "457954\n",
      "503749\n",
      "549544\n",
      "595340\n",
      "641135\n",
      "686931\n",
      "732726\n",
      "778521\n",
      "824317\n",
      "870112\n",
      "915908\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "def get_day(x):\n",
    "    return [j for (j, d) in enumerate(x[1:8]) if d == 1][0]\n",
    "\n",
    "def get_hour(x):\n",
    "    return [j for (j, h) in enumerate(x[8:32]) if h == 1][0]\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    user_devs = defaultdict(list)\n",
    "    sr_devs = defaultdict(list)\n",
    "    day_devs = defaultdict(list)\n",
    "    hour_devs = defaultdict(list)\n",
    "    \n",
    "    # Get deviation lists\n",
    "    for (i,x) in enumerate(Xtr):\n",
    "        dev = Ytr[i] - full_mean\n",
    "        user_devs[user_ids[idx[i]]].append(dev)\n",
    "\n",
    "        subreddit = x[0]\n",
    "        sr_devs[subreddit].append(dev)\n",
    "        \n",
    "        day = get_day(x)\n",
    "        day_devs[day].append(dev)\n",
    "        \n",
    "        hour = get_hour(x)\n",
    "        hour_devs[hour].append(dev)\n",
    "        \n",
    "    # Take means of lists\n",
    "    user_dev_means = {k: np.mean(v) for (k,v) in user_devs.iteritems()}\n",
    "    sr_dev_means = {k: np.mean(v) for (k,v) in sr_devs.iteritems()}\n",
    "    day_dev_means = {k: np.mean(v) for (k,v) in day_devs.iteritems()}\n",
    "    hour_dev_means = {k: np.mean(v) for (k,v) in hour_devs.iteritems()}\n",
    "    \n",
    "    # Make prediction as y = full_mean + (mean devs for user, hour, sr, day)\n",
    "    train_prediction = np.zeros(s)\n",
    "    for (i, x) in enumerate(Xtr):\n",
    "        prediction = full_mean + user_dev_means[user_ids[idx[i]]]\\\n",
    "                     + sr_dev_means[x[0]] + day_dev_means[get_day(x)]\\\n",
    "                     + hour_dev_means[get_hour(x)]\n",
    "        train_prediction[i] = prediction\n",
    "\n",
    "    test_prediction = np.zeros(testY.size)\n",
    "    for (i, x) in enumerate(testX):\n",
    "        uid = user_ids[idx[i+trainstop]]\n",
    "        prediction = full_mean + user_dev_means.get(uid,0) + sr_dev_means.get(uid,0)\\\n",
    "                     + day_dev_means.get(get_day(x),0) + hour_dev_means.get(get_hour(x),0)\n",
    "        test_prediction[i] = prediction\n",
    "        \n",
    "    trainerrs.append(np.sqrt(np.mean((train_prediction - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((test_prediction - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Means and deviations')\n",
    "plt.legend()\n",
    "plt.savefig('plots/mean_and_deviations.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try out a linear model using lasso regression with cross-validation to select regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45795\n",
      "91590\n",
      "137386\n",
      "183181\n",
      "228977\n",
      "274772\n",
      "320567\n",
      "366363\n",
      "412158\n",
      "457954\n",
      "503749\n",
      "549544\n",
      "595340\n",
      "641135\n",
      "686931\n",
      "732726\n",
      "778521\n",
      "824317\n",
      "870112\n",
      "915908\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    lasso_model = sklearn.linear_model.LassoCV(n_jobs=-1)\n",
    "    lasso_model.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((lasso_model.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((lasso_model.predict(testX) - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Lasso')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_lasso.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a random forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45795\n",
      "91590\n",
      "137386\n",
      "183181\n",
      "228977\n",
      "274772\n",
      "320567\n",
      "366363\n",
      "412158\n",
      "457954\n",
      "503749\n",
      "549544\n",
      "595340\n",
      "641135\n",
      "686931\n",
      "732726\n",
      "778521\n",
      "824317\n",
      "870112\n",
      "915908\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    rfmodel = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, max_features='auto', max_depth=10)\n",
    "    rfmodel.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((rfmodel.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((rfmodel.predict(testX) - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Random forest')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_randforest.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient boosting with randomized hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915908\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 31.9min finished\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "trainerrs = []\n",
    "testerrs = []\n",
    "xgb_models = [None] * len(trainsizes)\n",
    "\n",
    "for (i, s) in enumerate(trainsizes[-1:]): # 90sec for 45k examples; 612sec for 274k examples\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    paramsearch = {'learning_rate': scipy.stats.uniform(loc=0.1, scale=0.2), # uniform on [0.1, 0.3]\n",
    "                   'max_depth': scipy.stats.binom(n=10, p=0.5), # centered on depth 5\n",
    "                   'gamma': scipy.stats.expon(scale=10.0), # minimum reduction in loss needed to make split in dec tree\n",
    "                   'subsample': scipy.stats.uniform(loc=0.5, scale=0.5),  # Fraction of examples sampled per tree\n",
    "                  }\n",
    "        \n",
    "    xgb_model = XGBRegressor()\n",
    "    cv = sklearn.model_selection.RandomizedSearchCV(xgb_model, param_distributions=paramsearch,\n",
    "                                                    n_iter=10, n_jobs=1, verbose=1)\n",
    "    cv.fit(Xtr, Ytr)\n",
    "    xgb_models[i] = cv.best_estimator_    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=2.8995607300538015, learning_rate=0.20679822794373906,\n",
      "       max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
      "       n_estimators=100, nthread=-1, objective='reg:linear', reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,\n",
      "       subsample=0.80533878449187257)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "133.74932982698814"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xgb_models[0])\n",
    "np.sqrt(np.mean((xgb_models[0].predict(testX) - testY)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(xgb_models[0], open('xgb_full.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Booster',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " 'apply',\n",
       " 'base_score',\n",
       " 'booster',\n",
       " 'colsample_bylevel',\n",
       " 'colsample_bytree',\n",
       " 'evals_result',\n",
       " 'fit',\n",
       " 'gamma',\n",
       " 'get_params',\n",
       " 'get_xgb_params',\n",
       " 'learning_rate',\n",
       " 'max_delta_step',\n",
       " 'max_depth',\n",
       " 'min_child_weight',\n",
       " 'missing',\n",
       " 'n_estimators',\n",
       " 'nthread',\n",
       " 'objective',\n",
       " 'predict',\n",
       " 'reg_alpha',\n",
       " 'reg_lambda',\n",
       " 'scale_pos_weight',\n",
       " 'score',\n",
       " 'seed',\n",
       " 'set_params',\n",
       " 'silent',\n",
       " 'subsample']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('xgb_full.pickle'))\n",
    "dir(xgb_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
