{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import copy\n",
    "import time\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Set Up\n",
    "# I/O files\n",
    "reddit_submissions_file = 'data/reddit_submissions_jan2012.txt'\n",
    "output_file = 'output/text_features.tsv'\n",
    "\n",
    "# NLP init\n",
    "sw = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linguistic Features\n",
    "\n",
    "We can gather some simple features for our model in a single pass over each title, including:\n",
    "- proportion of \"good\" words\n",
    "- proportion of \"bad\" words\n",
    "- proportion of \"subreddit-specific\" words\n",
    "- \\# words for each part of speech\n",
    "- sentiment of title\n",
    "- is title > 16 tokens?\n",
    "- is title < 4 tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "\n",
    "def cleanAndStemTitle(title):\n",
    "    return [str(stemmer.stem(t)) for t in nltk.wordpunct_tokenize(title)\n",
    "                             if t.isalnum() and t not in sw]\n",
    "\n",
    "def cleanTitle(title):\n",
    "    return nltk.wordpunct_tokenize(title)\n",
    "\n",
    "def getNumTokens(title):\n",
    "    return len(' '.split(title))\n",
    "\n",
    "posTypes = ['determiner', 'pronoun', 'noun', 'adjective',\n",
    "           'adverb', 'interjection', 'preposition']\n",
    "posPresencesTemplate = {'has_{}'.format(t): 0 for t in posTypes}\n",
    "# See https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "posTagToTypeMap = {\n",
    "    'DT': 'determiner',\n",
    "    'PDT': 'determiner',\n",
    "    'WDT': 'determiner',\n",
    "    'PRP': 'pronoun',\n",
    "    'PRP$': 'pronoun',\n",
    "    'WP': 'pronoun',\n",
    "    'WP$': 'pronoun',\n",
    "    'NN': 'noun',\n",
    "    'NNS': 'noun',\n",
    "    'NNP': 'noun',\n",
    "    'NNPS': 'noun',\n",
    "    'JJ': 'adjective',\n",
    "    'JJR': 'adjective',\n",
    "    'JJS': 'adjective',\n",
    "    'RB': 'adverb',\n",
    "    'RBR': 'adverb',\n",
    "    'RBS': 'adverb',\n",
    "    'WRB': 'adverb',\n",
    "    'UH': 'interjection',\n",
    "    'IN': 'preposition',\n",
    "}\n",
    "\n",
    "def getPosPresenceFeatures(clean_title_tokens):\n",
    "    posPresences = copy.deepcopy(posPresencesTemplate)\n",
    "    tagged_tokens = pos_tag(clean_title_tokens)\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag not in posTagToTypeMap:\n",
    "            continue\n",
    "        posPresences['has_' + posTagToTypeMap[tag]] = 1\n",
    "    return posPresences\n",
    "\n",
    "def getLengthFeatures(raw_title):\n",
    "    features, num_tokens = {}, len(raw_title.split(' '))\n",
    "    if num_tokens > 16:\n",
    "        features['is_long_title'] = 1\n",
    "    else: \n",
    "        features['is_long_title'] = 0\n",
    "    if num_tokens < 4:\n",
    "        features['is_short_title'] = 1\n",
    "    else:\n",
    "        features['is_short_title'] = 0\n",
    "    return features\n",
    "\n",
    "def getSentimentFeature(raw_title):\n",
    "    features, polarity = {}, sid.polarity_scores(raw_title)['compound']\n",
    "    if polarity > 0.5:\n",
    "        features['sentiment'] = 1\n",
    "    elif polarity < -0.5:\n",
    "        features['sentiment'] = -1\n",
    "    else:\n",
    "        features['sentiment'] = 0\n",
    "    return features\n",
    "\n",
    "# Header for CSV file\n",
    "header = ['post_id', 'sentiment', 'is_long_title', 'is_short_title', \n",
    "          'has_determiner', 'has_pronoun', 'has_noun', 'has_adjective',\n",
    "          'has_adverb', 'has_interjection', 'has_preposition']\n",
    "# Object map to feature vector\n",
    "def getFeatureVectorFromFeatures(post_id, features):\n",
    "    # Normal row\n",
    "    featureV = []\n",
    "    featureV.append(post_id)\n",
    "    featureV.append(features['sentiment'])\n",
    "    featureV.append(features['is_long_title'])\n",
    "    featureV.append(features['is_short_title'])\n",
    "    featureV.append(features['has_determiner'])\n",
    "    featureV.append(features['has_pronoun'])\n",
    "    featureV.append(features['has_noun'])\n",
    "    featureV.append(features['has_adjective'])\n",
    "    featureV.append(features['has_adverb'])\n",
    "    featureV.append(features['has_interjection'])\n",
    "    featureV.append(features['has_preposition'])\n",
    "    return featureV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 done, (82.243857 elapsed)\n",
      "100000 done, (166.108537 elapsed)\n",
      "150000 done, (251.480932 elapsed)\n",
      "200000 done, (339.156273 elapsed)\n",
      "250000 done, (419.584445 elapsed)\n",
      "300000 done, (502.564954 elapsed)\n",
      "350000 done, (584.122533 elapsed)\n",
      "400000 done, (664.496846 elapsed)\n",
      "450000 done, (745.924309 elapsed)\n",
      "500000 done, (827.004177 elapsed)\n",
      "550000 done, (908.188698 elapsed)\n",
      "600000 done, (988.220124 elapsed)\n",
      "650000 done, (1069.092932 elapsed)\n",
      "700000 done, (1151.403407 elapsed)\n",
      "750000 done, (1234.733307 elapsed)\n",
      "800000 done, (1317.488206 elapsed)\n",
      "850000 done, (1400.332227 elapsed)\n",
      "900000 done, (1483.803302 elapsed)\n",
      "950000 done, (1565.460525 elapsed)\n",
      "1000000 done, (1648.073636 elapsed)\n",
      "1050000 done, (1729.447011 elapsed)\n",
      "1100000 done, (1813.645484 elapsed)\n",
      "1150000 done, (1896.760631 elapsed)\n",
      "1200000 done, (1980.596823 elapsed)\n",
      "1250000 done, (2062.273566 elapsed)\n",
      "1300000 done, (2143.396331 elapsed)\n",
      "1350000 done, (2225.690177 elapsed)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over submissions line by line, (potentially) outputting a\n",
    "# line in the output_file for each title.\n",
    "with open(reddit_submissions_file,'rb') as tsvin, open(output_file, 'wb') as csvout:\n",
    "    tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "    csvout = csv.writer(csvout)\n",
    "        \n",
    "    it = 0\n",
    "    start = time.time()\n",
    "    for row in tsvin:\n",
    "        if it == 0:\n",
    "            it += 1\n",
    "            continue\n",
    "        # if it > 100:\n",
    "        #     break\n",
    "        if it > 0 and it % 50000 == 0:\n",
    "            now = time.time()\n",
    "            print \"%d done, (%f elapsed)\" % (it, now - start)\n",
    "\n",
    "        # Where we'll be storing all the features for the title\n",
    "        features = {}\n",
    "\n",
    "        # Pull out the raw title text from csv row\n",
    "        raw_title = row[-2]\n",
    "        # Clean up title text\n",
    "        clean_title_tokens = cleanTitle(raw_title) # For POS tagging\n",
    "        clean_stemmed_title_tokens = cleanAndStemTitle(raw_title) # Stemmed for good/bad words\n",
    "        \n",
    "        # Feature 1: Pos presence features\n",
    "        features.update(getPosPresenceFeatures(clean_title_tokens)) \n",
    "        # Feature 2: Length features\n",
    "        features.update(getLengthFeatures(raw_title))\n",
    "        # Feature 3: Sentiment analysis\n",
    "        features.update(getSentimentFeature(raw_title))\n",
    "\n",
    "        # Convert features to feature vector\n",
    "        csvout.writerow(getFeatureVectorFromFeatures(row[0], features))\n",
    "        \n",
    "        it += 1\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
