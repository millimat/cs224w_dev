{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.linear_model, sklearn.ensemble, sklearn.model_selection \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srname_to_class = {}\n",
    "for (i,line) in enumerate(open('output/srurls_to_names.txt')):\n",
    "    url = line.strip().split()[0]\n",
    "    srname = url[3:-1] # e.g. '/r/politics/' to 'politics'\n",
    "    srname_to_class[srname] = np.float64(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids = [line.strip().split('\\t')[1] for line in \n",
    "            open('basic_language_network_structonly_nodelete.tsv').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = np.genfromtxt('output/basic_language_network_structonly_nodelete.tsv', delimiter='\\t', skip_header=1,\n",
    "#                      converters = {2: lambda name: srname_to_class[name]})\n",
    "\n",
    "# for line in open('basic_language_network_structonly_nodelete.tsv'):\n",
    "#     f = line.strip().split('\\t')[2]\n",
    "#     if f.startswith('-0'):\n",
    "#         print(f)\n",
    "#         print(line)\n",
    "\n",
    "data = pd.read_csv('basic_language_network_structonly_nodelete.tsv', delimiter='\\t').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = data.shape[0]\n",
    "idx = np.array(range(m), dtype=int)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "data = data[idx,:]\n",
    "\n",
    "# Create training and test set\n",
    "trainprop = 0.95\n",
    "trainstop = int(m * trainprop)\n",
    "\n",
    "# ignore gold column at end and post id/user id columns at beginning\n",
    "trainset = data[:trainstop]\n",
    "trainX = trainset[:,3:-2].astype(np.float64)\n",
    "trainY = trainset[:,-2].astype(np.float64)\n",
    "\n",
    "testset = data[trainstop:]\n",
    "testX = testset[:,3:-2].astype(np.float64)\n",
    "testY = testset[:,-2].astype(np.float64)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t3_opg76' 'chili_cheese_dog' 'nfl' ..., 0.361772 1.0 0.0]\n",
      " ['t3_o4sis' 'kevhurley' 'adviceanimals' ..., 0.187046 7.0 0.0]\n",
      " ['t3_o4wpo' 'philperspective' 'worldnews' ..., 0.394039 3.0 0.0]\n",
      " ..., \n",
      " ['t3_p3yf8' 'atr0292' 'funny' ..., 0.491966 1.0 0.0]\n",
      " ['t3_ol7q8' 'gforce917' 'tf2' ..., 0.527536 1.0 0.0]\n",
      " ['t3_oohno' 'gn4r-p0w' 'trees' ..., 0.638529 3.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(np.isnan(trainX)))\n",
    "print(np.argwhere(np.isnan(trainY)))\n",
    "print(np.argwhere(np.isnan(testX)))\n",
    "print(np.argwhere(np.isnan(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX = np.delete(trainX, [663245,780210], 0)\n",
    "trainY = np.delete(trainY, [663245,780210], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainsizes = np.array(np.linspace(0, trainstop, 21)[1:], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first baseline model, we'll just make predictions using the overall mean sore from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    trainerrs.append(np.sqrt(np.mean((full_mean - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((full_mean - testY)**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Mean-only')\n",
    "plt.legend()\n",
    "plt.savefig('plots/mean_only.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll track the average deviation of post score from the overall mean for each user, each hour, each day of the week, and each subreddit. Each prediction will be the overall mean plus the sum of mean deviations for each relevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "def get_day(x):\n",
    "    return [j for (j, d) in enumerate(x[1:8]) if d == 1][0]\n",
    "\n",
    "def get_hour(x):\n",
    "    return [j for (j, h) in enumerate(x[8:32]) if h == 1][0]\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    user_devs = defaultdict(list)\n",
    "    sr_devs = defaultdict(list)\n",
    "    day_devs = defaultdict(list)\n",
    "    hour_devs = defaultdict(list)\n",
    "    \n",
    "    # Get deviation lists\n",
    "    for (i,x) in enumerate(Xtr):\n",
    "        dev = Ytr[i] - full_mean\n",
    "        user_devs[user_ids[idx[i]]].append(dev)\n",
    "\n",
    "        subreddit = x[0]\n",
    "        sr_devs[subreddit].append(dev)\n",
    "        \n",
    "        day = get_day(x)\n",
    "        day_devs[day].append(dev)\n",
    "        \n",
    "        hour = get_hour(x)\n",
    "        hour_devs[hour].append(dev)\n",
    "        \n",
    "    # Take means of lists\n",
    "    user_dev_means = {k: np.mean(v) for (k,v) in user_devs.iteritems()}\n",
    "    sr_dev_means = {k: np.mean(v) for (k,v) in sr_devs.iteritems()}\n",
    "    day_dev_means = {k: np.mean(v) for (k,v) in day_devs.iteritems()}\n",
    "    hour_dev_means = {k: np.mean(v) for (k,v) in hour_devs.iteritems()}\n",
    "    \n",
    "    # Make prediction as y = full_mean + (mean devs for user, hour, sr, day)\n",
    "    train_prediction = np.zeros(s)\n",
    "    for (i, x) in enumerate(Xtr):\n",
    "        prediction = full_mean + user_dev_means[user_ids[idx[i]]]\\\n",
    "                     + sr_dev_means[x[0]] + day_dev_means[get_day(x)]\\\n",
    "                     + hour_dev_means[get_hour(x)]\n",
    "        train_prediction[i] = prediction\n",
    "\n",
    "    test_prediction = np.zeros(testY.size)\n",
    "    for (i, x) in enumerate(testX):\n",
    "        uid = user_ids[idx[i+trainstop]]\n",
    "        prediction = full_mean + user_dev_means.get(uid,0) + sr_dev_means.get(uid,0)\\\n",
    "                     + day_dev_means.get(get_day(x),0) + hour_dev_means.get(get_hour(x),0)\n",
    "        test_prediction[i] = prediction\n",
    "        \n",
    "    trainerrs.append(np.sqrt(np.mean((train_prediction - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((test_prediction - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Means and deviations')\n",
    "plt.legend()\n",
    "plt.savefig('plots/mean_and_deviations.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try out a linear model using lasso regression with cross-validation to select regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    lasso_model = sklearn.linear_model.LassoCV(n_jobs=-1)\n",
    "    lasso_model.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((lasso_model.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((lasso_model.predict(testX) - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Lasso')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_lasso.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a random forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    rfmodel = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, max_features='auto', max_depth=10)\n",
    "    rfmodel.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((rfmodel.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((rfmodel.predict(testX) - testY)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Random forest')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_randforest.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient boosting with randomized hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting model\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'enumerate' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-b08bfc93288e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'enumerate' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "# Do a randomized CV search for hyperparameters, then use these for the rest of training\n",
    "s = trainsizes[0]\n",
    "Xtr = trainX[:s,:]\n",
    "Ytr = trainY[:s]\n",
    "paramsearch = {'learning_rate': scipy.stats.uniform(loc=0.1, scale=0.2), # uniform on [0.1, 0.3]\n",
    "               'max_depth': scipy.stats.binom(n=10, p=0.5), # centered on depth 5\n",
    "               'gamma': scipy.stats.expon(scale=10.0), # minimum reduction in loss needed to make split in dec tree\n",
    "               'subsample': scipy.stats.uniform(loc=0.5, scale=0.5),  # Fraction of examples sampled per tree\n",
    "              }\n",
    "xgb_model = XGBRegressor()\n",
    "print('Selecting model')\n",
    "cv = sklearn.model_selection.RandomizedSearchCV(xgb_model, param_distributions=paramsearch,\n",
    "                                                n_iter=5, n_jobs=1, verbose=1)\n",
    "cv.fit(Xtr, Ytr)\n",
    "params = cv.best_params_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39235\n",
      "78471\n",
      "117707\n",
      "156943\n",
      "196179\n",
      "235415\n",
      "274651\n",
      "313887\n",
      "353123\n",
      "392359\n",
      "431595\n",
      "470831\n",
      "510067\n",
      "549303\n",
      "588539\n",
      "627775\n",
      "667011\n",
      "706247\n",
      "745483\n",
      "784719\n",
      "[133.92704154180117, 113.3132499550499, 121.31774233606453, 125.63368121981328, 127.54174116114419, 129.50234186121116, 130.50148169099208, 130.65643870361845, 130.99431750932749, 131.35142000961466, 132.00388022770201, 133.13598541367529, 132.81465809126954, 132.64321318236506, 133.16980248202597, 133.32501486954331, 133.04532735025194, 132.89302160349439, 133.23573307645142, 133.67873204604774, 133.92704154180117]\n",
      "[137.08038874851025, 140.72081379202223, 139.90408169027859, 138.86786441269194, 138.54379766066117, 138.34546906044218, 138.08806968767038, 137.4371946428729, 137.58795099283307, 137.66838930955274, 137.27412362150346, 137.10963387317869, 137.39524953553749, 137.06537524751735, 137.26353731810889, 137.43780137063908, 137.31064055815031, 137.10757147300265, 137.28998475747875, 137.04922538097904, 137.08038874851025]\n"
     ]
    }
   ],
   "source": [
    "for (i, s) in enumerate(trainsizes):\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    xgb_model = cv.best_estimator_\n",
    "    xgb_model.fit(Xtr, Ytr)\n",
    "    \n",
    "    trainerrs.append(np.sqrt(np.mean((xgb_model.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((xgb_model.predict(testX) - testY)**2)))\n",
    "\n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Boosted trees')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_network_xgboost.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cv.best_estimator_, open('xgb_network.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.load(open('xgb_full.pickle'))\n",
    "dir(xgb_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
