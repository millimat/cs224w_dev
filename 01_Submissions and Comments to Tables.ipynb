{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import snap\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = 1000\n",
    "def filter_subreddits(subreddit_metagraph):\n",
    "    mg = snap.TNEANet.Load(snap.TFIn(subreddit_metagraph))\n",
    "    srids_to_names = {}\n",
    "\n",
    "    # Get the largest |top| subreddits by subscriber count\n",
    "    acceptable = sorted(((NI.GetId(), mg.GetIntAttrDatN(NI, 'subscribers')) for NI in mg.Nodes()),\n",
    "                        key=(lambda (nid, subs): subs), reverse=True)[:top]\n",
    "    acceptable = set(nid for (nid, subs) in acceptable)\n",
    "\n",
    "    for NI in mg.Nodes():\n",
    "        # t5_xxxxx -> politics\n",
    "        if NI.GetId() in acceptable:\n",
    "            srids_to_names[mg.GetStrAttrDatN(NI, 'name')] = mg.GetStrAttrDatN(NI, 'url')[3:-1]\n",
    "\n",
    "    return srids_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit_metagraph = 'output/subreddits.graph'\n",
    "srids_to_srnames = filter_subreddits(subreddit_metagraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_submissions(srids_to_srnames, submission_directory, outsubname):\n",
    "    outfile = open(outsubname, 'w')\n",
    "    outfile.write('#' + '\\t'.join(['Post_ID', 'Author', 'Subreddit', 'Score', 'Gilded', 'Created_UTC', 'Is_Self',\n",
    "                           'Title', 'Selftext']) + '\\n')\n",
    "\n",
    "    postids_to_authors = {}\n",
    "\n",
    "    subfiles = []\n",
    "    for fname in os.listdir(submission_directory):\n",
    "        if fname.startswith('.') or fname.endswith('~'):\n",
    "            continue\n",
    "        path = os.path.join(submission_directory, fname)\n",
    "        if os.path.isdir(path): # skip subdirectories\n",
    "            continue\n",
    "        subfiles.append(path)\n",
    "    \n",
    "    # Iterate over posts, add to table\n",
    "    print('Parsing submissions...')\n",
    "    for subfile in subfiles:\n",
    "        print(subfile + ': parsing...')\n",
    "        submissions = (json.loads(line) for line in bz2.BZ2File(subfile))\n",
    "        for (i, sub) in enumerate(submissions):\n",
    "            if sub['subreddit_id'] in srids_to_srnames: # subreddit_id is t5_xxxxx\n",
    "                post_id = 't3_' + sub['id']\n",
    "                author = sub['author'].lower()\n",
    "                subreddit = srids_to_srnames[sub['subreddit_id']]\n",
    "                score = sub['score']\n",
    "                gold = sub.get('gilded', 0)\n",
    "                timestamp = sub['created_utc']\n",
    "                is_self = int(sub['is_self'])\n",
    "                \n",
    "                # Regex converts all whitespace to single space to avoid line breaks or tabs\n",
    "                title = re.sub(r'\\s+', ' ', sub['title'].encode('ascii', 'backslashreplace')).lower()\n",
    "                selftext = '' if not is_self \\\n",
    "                           else re.sub(r'\\s+', ' ', sub['selftext'].encode('ascii', 'backslashreplace')).lower()\n",
    "\n",
    "                postids_to_authors[post_id] = author\n",
    "                outfile.write('\\t'.join(str(x) for x in [post_id, author, subreddit, score, gold, timestamp,\n",
    "                                                         is_self, title, selftext]) + '\\n')\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(i)\n",
    "                \n",
    "    outfile.close()\n",
    "\n",
    "    return postids_to_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_directory = 'data/submissions_jan2012'\n",
    "output_submission_text_file = 'output/reddit_submissions_jan2012.txt'\n",
    "postids_to_authors = parse_submissions(srids_to_srnames, submission_directory, output_submission_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_comments(srids_to_srnames, postids_to_authors, comment_directory, outcomname):\n",
    "    outfile = open(outcomname, 'w')\n",
    "    outfile.write('#Commenter\\tCommentee\\tSubreddit\\tComment_ID\\tPost_ID\\tParent_ID\\tControversiality\\tUpvotes\\t' + \n",
    "                  'Downvotes\\tScore\\tGilded\\tCreated_UTC\\tIs_Reply\\tAuthor_Deleted\\t' + \n",
    "                  'Text_Deleted\\tComment_Text\\n')\n",
    "    \n",
    "    comids_to_authors = {} # t1_xxxxx -> gallowboob, e.g.\n",
    "        \n",
    "    comfiles = []\n",
    "    for fname in os.listdir(comment_directory):\n",
    "        if fname.startswith('.') or fname.endswith('~'):\n",
    "            continue\n",
    "        path = os.path.join(comment_directory, fname)\n",
    "        if os.path.isdir(path): # skip subdirectories\n",
    "            continue\n",
    "        comfiles.append(path)\n",
    "        \n",
    "    print('Parsing comments...')\n",
    "    for comfile in comfiles:        \n",
    "        print(comfile + ': parsing...')\n",
    "        comments = (json.loads(line) for line in bz2.BZ2File(comfile))\n",
    "        for (i, com) in enumerate(comments):\n",
    "            sr_id = com['subreddit_id'] # t5_xxxxx\n",
    "            post_id = com['link_id'] # t3_xxxxx\n",
    "            parent_id = com['parent_id'] # t1_xxxxx (comment) if reply; t3_xxxxx (post) if top-level comment\n",
    "            comment_id = com['name'] # t1_xxxxx\n",
    "            is_reply = parent_id.startswith('t1')\n",
    "            \n",
    "            # Ignore subreddits we don't care about, posts we haven't seen, and replies to comments we haven't seen\n",
    "            if sr_id in srids_to_srnames and post_id in postids_to_authors\\\n",
    "            and (not is_reply or parent_id in comids_to_authors):\n",
    "                sr_name = srids_to_srnames[sr_id]\n",
    "                commenter = com['author'].lower()\n",
    "                parent_dict = comids_to_authors if is_reply else postids_to_authors\n",
    "                commentee = parent_dict[parent_id]\n",
    "                controversiality = com['controversiality']\n",
    "                upvotes = com['ups']\n",
    "                downvotes = com['downs']\n",
    "                score = com['score']\n",
    "                gilded = com['gilded']\n",
    "                created = com['created_utc']\n",
    "                body = re.sub(r\"\\s+\", ' ', com['body'].encode('ascii', 'backslashreplace')).lower()\n",
    "                author_deleted = int(commenter == '[deleted]')\n",
    "                text_deleted = int(body in ('[deleted]', '[removed]'))\n",
    "                \n",
    "                outfile.write('\\t'.join(str(x) for x in \n",
    "                              [commenter, commentee, sr_name, comment_id, post_id, parent_id, controversiality, \n",
    "                               upvotes, downvotes, score, gilded, created, int(is_reply), author_deleted,\n",
    "                               text_deleted, body]) + '\\n')\n",
    "                \n",
    "                comids_to_authors[com['name']] = commenter\n",
    "            \n",
    "            if i % 100000 == 0:\n",
    "                print(i) # Rudimentary progress indicator\n",
    "                \n",
    "    return comids_to_authors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_directory = 'data/comments_jan2012'\n",
    "output_comment_text_file = 'output/reddit_comments_jan2012.txt'\n",
    "comids_to_authors = parse_comments(srids_to_srnames, postids_to_authors, comment_directory, output_comment_text_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
