{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn.linear_model, sklearn.ensemble, sklearn.model_selection \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "np.random.seed(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srname_to_class = {}\n",
    "for (i,line) in enumerate(open('output/srurls_to_names.txt')):\n",
    "    url = line.strip().split()[0]\n",
    "    srname = url[3:-1] # e.g. '/r/politics/' to 'politics'\n",
    "    srname_to_class[srname] = np.float64(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids = [line.strip().split('\\t')[1] for line in open('output/basic_and_language_nodelete.tsv').readlines()[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_basic_language = np.genfromtxt('output/basic_and_language_nodelete.tsv', delimiter='\\t', skip_header=1,\n",
    "                                   converters = {2: lambda name: srname_to_class[name]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = data_basic_language.shape[0]\n",
    "idx = np.array(range(m), dtype=int)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "data_basic_language = data_basic_language[idx,:]\n",
    "\n",
    "# Create training and test set\n",
    "trainprop = 0.95\n",
    "trainstop = int(m * trainprop)\n",
    "\n",
    "# ignore gold column at end and post id/user id columns at beginning\n",
    "trainset = data_basic_language[:trainstop]\n",
    "trainX = trainset[:,2:-2] \n",
    "trainY = trainset[:,-2]\n",
    "\n",
    "testset = data_basic_language[trainstop:]\n",
    "testX = testset[:,2:-2]\n",
    "testY = testset[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainsizes = np.array(np.linspace(0, trainstop, 21)[1:], dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first baseline model, we'll just make predictions using the overall mean sore from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915908\n",
      "[139.6966371604569]\n",
      "[136.64446859815664]\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes[-1:]:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    trainerrs.append(np.sqrt(np.mean((full_mean - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((full_mean - testY)**2)))\n",
    "    \n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "# plt.plot(trainsizes, testerrs, label='Test err')\n",
    "# plt.xlabel('Training set size')\n",
    "# plt.ylabel('RMSE')\n",
    "# plt.title('Mean-only')\n",
    "# plt.legend()\n",
    "# plt.savefig('plots/mean_only.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll track the average deviation of post score from the overall mean for each user, each hour, each day of the week, and each subreddit. Each prediction will be the overall mean plus the sum of mean deviations for each relevant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915908\n",
      "[107.32957158841134]\n",
      "[145.88987686286163]\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "def get_day(x):\n",
    "    return [j for (j, d) in enumerate(x[1:8]) if d == 1][0]\n",
    "\n",
    "def get_hour(x):\n",
    "    return [j for (j, h) in enumerate(x[8:32]) if h == 1][0]\n",
    "\n",
    "for s in trainsizes[-1:]:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    full_mean = np.mean(Ytr)\n",
    "    \n",
    "    user_devs = defaultdict(list)\n",
    "    sr_devs = defaultdict(list)\n",
    "    day_devs = defaultdict(list)\n",
    "    hour_devs = defaultdict(list)\n",
    "    \n",
    "    # Get deviation lists\n",
    "    for (i,x) in enumerate(Xtr):\n",
    "        dev = Ytr[i] - full_mean\n",
    "        user_devs[user_ids[idx[i]]].append(dev)\n",
    "\n",
    "        subreddit = x[0]\n",
    "        sr_devs[subreddit].append(dev)\n",
    "        \n",
    "        day = get_day(x)\n",
    "        day_devs[day].append(dev)\n",
    "        \n",
    "        hour = get_hour(x)\n",
    "        hour_devs[hour].append(dev)\n",
    "        \n",
    "    # Take means of lists\n",
    "    user_dev_means = {k: np.mean(v) for (k,v) in user_devs.iteritems()}\n",
    "    sr_dev_means = {k: np.mean(v) for (k,v) in sr_devs.iteritems()}\n",
    "    day_dev_means = {k: np.mean(v) for (k,v) in day_devs.iteritems()}\n",
    "    hour_dev_means = {k: np.mean(v) for (k,v) in hour_devs.iteritems()}\n",
    "    \n",
    "    # Make prediction as y = full_mean + (mean devs for user, hour, sr, day)\n",
    "    train_prediction = np.zeros(s)\n",
    "    for (i, x) in enumerate(Xtr):\n",
    "        prediction = full_mean + user_dev_means[user_ids[idx[i]]]\\\n",
    "                     + sr_dev_means[x[0]] + day_dev_means[get_day(x)]\\\n",
    "                     + hour_dev_means[get_hour(x)]\n",
    "        train_prediction[i] = prediction\n",
    "\n",
    "    test_prediction = np.zeros(testY.size)\n",
    "    for (i, x) in enumerate(testX):\n",
    "        uid = user_ids[idx[i+trainstop]]\n",
    "        prediction = full_mean + user_dev_means.get(uid,0) + sr_dev_means.get(uid,0)\\\n",
    "                     + day_dev_means.get(get_day(x),0) + hour_dev_means.get(get_hour(x),0)\n",
    "        test_prediction[i] = prediction\n",
    "        \n",
    "    trainerrs.append(np.sqrt(np.mean((train_prediction - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((test_prediction - testY)**2)))\n",
    "    \n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-66694d007a49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainerrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train err'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainsizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesterrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test err'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training set size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3238\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3239\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3240\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3241\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3242\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Matthew/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_base.pyc\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (1,)"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Means and deviations')\n",
    "plt.legend()\n",
    "plt.savefig('plots/mean_and_deviations.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try out a linear model using lasso regression with cross-validation to select regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915908\n",
      "[139.2116192855934]\n",
      "[136.18329376500694]\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes[-1:]:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    lasso_model = sklearn.linear_model.LassoCV(n_jobs=-1)\n",
    "    lasso_model.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((lasso_model.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((lasso_model.predict(testX) - testY)**2)))\n",
    "    \n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "# plt.plot(trainsizes, testerrs, label='Test err')\n",
    "# plt.xlabel('Training set size')\n",
    "# plt.ylabel('Error')\n",
    "# plt.title('Lasso')\n",
    "# plt.legend()\n",
    "# plt.savefig('plots/basic_language_lasso.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a random forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915908\n",
      "[134.24033313710825]\n",
      "[134.44745195141945]\n"
     ]
    }
   ],
   "source": [
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "for s in trainsizes[-1:]:\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    \n",
    "    rfmodel = sklearn.ensemble.RandomForestRegressor(n_jobs=-1, max_features='auto', max_depth=10)\n",
    "    rfmodel.fit(Xtr, Ytr)\n",
    "    trainerrs.append(np.sqrt(np.mean((rfmodel.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((rfmodel.predict(testX) - testY)**2)))\n",
    "\n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "# plt.plot(trainsizes, testerrs, label='Test err')\n",
    "# plt.xlabel('Training set size')\n",
    "# plt.ylabel('Error')\n",
    "# plt.title('Random forest')\n",
    "# plt.legend()\n",
    "# plt.savefig('plots/basic_language_randforest.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Gradient boosting with randomized hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45795\n",
      "91590\n",
      "137386\n",
      "183181\n",
      "228977\n",
      "274772\n",
      "320567\n",
      "366363\n",
      "412158\n",
      "457954\n",
      "503749\n",
      "549544\n",
      "595340\n",
      "641135\n",
      "686931\n",
      "732726\n",
      "778521\n",
      "824317\n",
      "870112\n",
      "915908\n",
      "[128.30427259624, 132.93122756142216, 132.81225540222732, 134.04573563145107, 134.14938926738407, 135.01492918249292, 135.11473145292663, 135.1324555007198, 135.95951592102315, 135.98717515876726, 136.05846616981955, 136.87330030358666, 136.87194672650017, 137.03889798290291, 137.18720595963231, 136.82728211348422, 136.33757274302837, 136.45352956416295, 136.54103004439796, 136.36981788627801]\n",
      "[134.9073120723078, 134.41134241051142, 134.32735474055789, 134.19390448286219, 134.14318193836982, 133.98034103586451, 134.23807299078013, 134.11236636701065, 133.96330055475349, 133.86116247983671, 133.944483031918, 133.85719554333937, 133.85060976568036, 133.89240984952332, 133.81540835397888, 133.84764546490587, 133.90180724457181, 133.83627349569761, 133.88756515294915, 133.8573866253316]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "trainerrs = []\n",
    "testerrs = []\n",
    "\n",
    "# Do a randomized CV search for hyperparameters, then use these for the rest of training\n",
    "s = trainsizes[0]\n",
    "Xtr = trainX[:s,:]\n",
    "Ytr = trainY[:s]\n",
    "paramsearch = {'learning_rate': scipy.stats.uniform(loc=0.1, scale=0.2), # uniform on [0.1, 0.3]\n",
    "               'max_depth': scipy.stats.binom(n=10, p=0.5), # centered on depth 5\n",
    "               'gamma': scipy.stats.expon(scale=10.0), # minimum reduction in loss needed to make split in dec tree\n",
    "               'subsample': scipy.stats.uniform(loc=0.5, scale=0.5),  # Fraction of examples sampled per tree\n",
    "              }\n",
    "xgb_model = XGBRegressor()\n",
    "cv = sklearn.model_selection.RandomizedSearchCV(xgb_model, param_distributions=paramsearch,\n",
    "                                                n_iter=10, n_jobs=-1, verbose=1)\n",
    "cv.fit(Xtr, Ytr)\n",
    "\n",
    "for (i, s) in enumerate(trainsizes):\n",
    "    print(s)\n",
    "    Xtr = trainX[:s,:]\n",
    "    Ytr = trainY[:s]\n",
    "    xgb_model = cv.best_estimator_\n",
    "    xgb_model.fit(Xtr, Ytr)\n",
    "    \n",
    "    trainerrs.append(np.sqrt(np.mean((xgb_model.predict(Xtr) - Ytr)**2)))\n",
    "    testerrs.append(np.sqrt(np.mean((xgb_model.predict(testX) - testY)**2)))\n",
    "    \n",
    "print(trainerrs)\n",
    "print(testerrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(trainsizes, trainerrs, label='Train err')\n",
    "plt.plot(trainsizes, testerrs, label='Test err')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Boosted trees')\n",
    "plt.legend()\n",
    "plt.savefig('plots/basic_language_xgboost.eps', format='eps', dpi=1000)\n",
    "\n",
    "# cv.best_estimator_\n",
    "# print(xgb_models[0])\n",
    "# np.sqrt(np.mean((xgb_models[0].predict(testX) - testY)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(xgb_models[0], open('xgb_full.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.load(open('xgb_full.pickle'))\n",
    "# dir(xgb_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
